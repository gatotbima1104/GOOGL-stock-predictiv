# -*- coding: utf-8 -*-
"""1-StudyKasus-Muhamad Gatot

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1buPnvJX7tsq-gpTpX_FMzNj2JwyWg_PR

# Muhamad Gatot Supiadin
## M183X0343 | M01 - Pengembangan Machine Learning dan Front End Web
Universitas Amikom Yogyakarta , Sleman Yogyakarta

**Import Libaries**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

from matplotlib import pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.impute import SimpleImputer

from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor

"""**Collecting Dataset**"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d varpit94/google-stock-data

!unzip '/content/google-stock-data.zip'

import pandas as pd

df = pd.read_csv('GOOGL.csv')
df.head(5)

df['Volume'] = df['Volume'].astype(float)

df.info()

"""# **Exploratory Data Analysis (EDA)**
**Description of variable** 


1.   Date : Opening rekap data
2.   High : Highest price per day
3.   Low  : Lowest price per day
4.   Open : Opening price per day
5.   Close  : Closing price per day
6.   Adj Close : Closing price per day after counting stock split or stock reverse
6.   Volume : Volume Transaction price per day

"""

print(f'The data has {df.shape[0]} records and {df.shape[1]} columns.')

"""**Pengecekan Missing Value**"""

df.isnull().sum()

print('Jumlah data yang null adalah ', df.isnull().sum().sum(), ' records')

"""# **Statistic Dataset Information**


* count adalah jumlah sampel pada dataset.
* mean adalah nilai rata-rata dataset.
* std adalah standar deviasi.
* min adalah nilai minimum.
* 25% adalah kuartil pertama.
* 50% adalah kuartil kedua.
* 75% adalah kuartil ketiga.
* max adalah nilai maksimum
"""

df.describe()

"""# **Data Visulaisation**

Untuk mengecek apakah ada outliers pada dataset
"""

numerical_col = [col for col in df.columns if df[col].dtypes == 'float64']
plt.subplots(figsize=(15,10))
sns.boxplot(data=df[numerical_col]).set_title("GOOGLE Stock Price")
plt.show()

"""Terlihat pada kolom Volume. Mengatasi outliner pada data menggunakan IQR Method dengan cara menghapus data yang berada diluar daripada IQR."""

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3-Q1
df = df[~((df<(Q1-1.5*IQR))|(df>(Q3+1.5*IQR))).any(axis=1)]

df.shape

print(f'Data consist of {df.shape[1]} columns')
print(f'Each column consists of {df.shape[0]} records')

"""Data diatas merupakan data yang sudah clean dari outliner

Berikut merupakan **Visualisasi data** dari dataset yang tidak memiliki outliner
"""

numerical_data = [col for col in df.columns if df[col].dtype == 'float64']
plt.figure(figsize=(20, 10))
sns.boxplot(data=df[numerical_data]).set_title('GOOGLE Stock Price ')
plt.show()

"""# **Univariate Analysis**
kolom yang ditargetkan adalah kolom 'Adj Close'
"""

cols = 3
rows = 2
fig = plt.figure(figsize=(cols * 5, rows * 5))

for i, col in enumerate(numerical_col):
  ax = fig.add_subplot(rows, cols, i + 1)
  sns.histplot(x=df[col], bins=30, kde=True, ax=ax)
fig.tight_layout()
plt.show()

"""# **Multivariate Analysis**
Setelah menargetkan kolom Adj Close, sekarang kita coba mengkorelasikan Adj Cluse terhadap fitur lain lalu kita simpulkan
"""

sns.pairplot(df[numerical_col], diag_kind='kde')
plt.show()

"""Dari data diatas dapat disimpulkan bahwa Adj Close memiliki korelasi positif yang kuat terhadap Open, High, Low dan Close. sedangkan pada kolom Volume memiliki korelasi yang tidak kuat

**Korelasi Heatmap**
"""

plt.figure(figsize=(15,8))
corr = df[numerical_col].corr().round(2)
sns.heatmap(data=corr, annot=True, vmin=-1, vmax=1, cmap='coolwarm', linewidth=1)
plt.title('Correlation matrix for numerical feature', size=15)
plt.show()

"""Pada proses Training model kita akan menghapus atau gunakan fungsi drop() untuk menghilangkan kolom yang akan mengganggu proses training model"""

GOOGL = df.drop(['Date', 'Volume', 'Close'], axis=1)
GOOGL.head()

"""# **Splitting Data**"""

X = GOOGL.iloc[:, :-1].values
y = GOOGL.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)

print('X_train has', len(X_train), 'records')
print('y_train has', len(y_train), 'records')
print('X_test has', len(X_test), 'records')
print('y_test has', len(y_test), 'records')

"""# **Normalization**
Untuk mempercepat training data kita akan normalisasikan dataset dimana nanti data akan diubah diantara rentan 0 hingga 1 menggunakan Minmaxscaler()
"""

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

models = pd.DataFrame(columns=['train_mse', 'test_mse'],
                      index=['SVR', 'KNN', 'GradientBoosting'])

"""# **Modelling**
Pada tahap ini kita menggunakan Hyperparameter tunning, dimana akan menambah model secara otomatis agar mencapai model terbaiknya.
kitapun dapat mengaturnya secara manual menggunakan GridSearch, dimana teknik ini akan menguji hyperparameter sekaligus pada model
"""

def grid_search(model, hyperparameters):
  results = GridSearchCV(
      model,
      hyperparameters,
      cv=5,
      verbose=1,
      n_jobs=6
  )

  return results

svr = SVR()
hyperparameters = {
    'kernel': ['rbf'],
    'C': [0.001, 0.01, 0.1, 10, 100, 1000],
    'gamma': [0.3, 0.03, 0.003, 0.0003]
}

svr_search = grid_search(svr, hyperparameters)
svr_search.fit(X_train, y_train)
print(svr_search.best_params_)
print(svr_search.best_score_)

knn = KNeighborsRegressor()
hyperparameters = {
    'n_neighbors': range(1, 10)
}

knn_search = grid_search(knn, hyperparameters)
knn_search.fit(X_train, y_train)
print(knn_search.best_params_)
print(knn_search.best_score_)

gradient_boost = GradientBoostingRegressor()
hyperparameters = {
    'learning_rate': [0.01, 0.001, 0.0001],
    'n_estimators': [250, 500, 750, 1000],
    'criterion': ['friedman_mse', 'squared_error']
}

gradient_boost_search = grid_search(gradient_boost, hyperparameters)
gradient_boost_search.fit(X_train, y_train)
print(gradient_boost_search.best_params_)
print(gradient_boost_search.best_score_)

"""# **Fitting Model**
Disini kita akan menggunakan 3 buah algoritma diantarnya SVR, KNN dan Gradient Boost.
"""

svr = SVR(C=1000, gamma=0.003, kernel='rbf')
svr.fit(X_train, y_train)

knn = KNeighborsRegressor(n_neighbors=6)
knn.fit(X_train, y_train)

gradient_boost = GradientBoostingRegressor(criterion='squared_error',
                                           learning_rate=0.01, n_estimators=1000)
gradient_boost.fit(X_train, y_train)

"""# **Model Evaluation**
Setelah menerapkan algritma pada model, selanjutnya kita bisa melihat MSE mana yang terkecil dari ketiga algortma yang dipakai
"""

model_dict = {
    'SVR': svr,
    'KNN': knn,
    'GradientBoosting': gradient_boost,
    
}

for name, model in model_dict.items():
  models.loc[name, 'train_mse'] = mean_squared_error(y_train, model.predict(X_train))
  models.loc[name, 'test_mse'] = mean_squared_error(y_test, model.predict(X_test))

models.head()

models.sort_values(by='test_mse', ascending=False).plot(kind='bar', zorder=3)

"""Pada Visual diatas bisa kita liat bahwa algoritma KNN adalah algortima yang memberikan MSE terendah, untuk lebih jelasnya kita akan mencari akurasi pada setiap algortma dengan bantuan score()"""

svr_acc = svr.score(X_test, y_test)*100
knn_acc = knn.score(X_test, y_test)*100
boosting_acc = gradient_boost.score(X_test, y_test)*100

evaluation_list = [[svr_acc], [knn_acc], [boosting_acc]]
evaluation = pd.DataFrame(evaluation_list,
                          columns = ['Accuracy (%)'],
                          index = ['SVR', 'KNN', 'Gradient Boost'])

evaluation

"""dari hasil evaluasi menunjukan bahwa KNN merupakan algortima terbaik untuk model

# **Forecasting Price**
pada proses ini kita akan coba memprediksikan harga dari saham GOOGLE dalam kurun waktu sebulan kedepan
"""

X_30=X[-30:]
forecast=knn.predict(X_30)

forecast=pd.DataFrame(forecast,columns=['Forecast'])
GOOGLE = GOOGL.append(forecast)
GOOGLE.drop(['High', 'Low', 'Open'],axis=1,inplace=True)

GOOGLE.tail(35)